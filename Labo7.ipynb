{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labo 7 Data Science: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong>Opmerking.</strong> In dit labo worden decision trees getekend a.d.h.v. het pakket GraphViz. In labo 4 &amp; 5 werd dit pakket geïnstalleerd. Op je eigen PC installeer je de package <strong>python-graphviz</strong> in Anaconda (onder environments). Dit kan een tijdje duren. Herstart je kernel om de installatie te kunnen gebruiken.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 1** : **Decision Trees voor een eenvoudig classificatieprobleem**\n",
    "\n",
    "**1.1 De data verkennen**\n",
    "\n",
    "Gegeven de dataset van housing-ny-sf.csv. Deze dataset kan worden gebruikt om te voorspellen of een appartement in New York gelegen is of in San Fransisco. Het bestand bevat volgende kolommen:\n",
    " * in_sf: het te voorspellen target: staat op 1 indien het appartement in San Francisco gelegen is\n",
    " * beds: het aantal bedden\n",
    " * bath: het aantal baden\n",
    " * price: de verkoopprijs (\\$)\n",
    " * year_built: het bouwjaar\n",
    " * sqft: de oppervlakte in square foot\n",
    " * price_per_sqft: de prijs (\\$) per square foot\n",
    " * elevation: hoogte in m\n",
    "\n",
    "Een leuke visuele intro op deze oefening vind je hier: _http://www.r2d3.us/visual-intro-to-machine-learning-part-1/_\n",
    "\n",
    " * Laad de data in in een Pandas-dataframe (gelieve niks te veranderen aan het csv-bestand, tip: skippen).\n",
    " * Maak een scatter_matrix-plot van de __features__ waarbij elke instanties steeds ingekleurd wordt volgens zijn target (met colormap 'brg' wordt San Francisco groen en New York blauw)\n",
    " * Teken met Pandas (groupby en hist(alpha=0.4)) een histogram (met verschillende kleur voor SF en NY) voor een aantal features waarvan je verwacht dat de spreiding voor de 2 steden sterk verschilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Training en parameter tuning**\n",
    "\n",
    " * Deel de data in in een trainingset en een test set (70%/30%) - kies een random_state &ne; 0 bv. 88\n",
    " * Train deze data met DecisionTreeClassifier zonder parameters\n",
    " * Schrijf een script dat de ideale diepte zoekt van de decision tree en teken deze tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oefening 2** : **Classificatie-oefening met decision trees, random forests en gradient boosting machines**\n",
    "\n",
    "**2.1 Generatie van de sample data**\n",
    "\n",
    "Je start deze oefening met de creatie van sample data voor een 'ternair classificatieprobleem'. Deze data heeft 2 features nl. X en Y, en een target genaamd __color__ (mogelijke waarde: red, green, blue). We zullen deze data gebruiken om met verschillende classificatie-algoritmen te testen en de decision boundary te visualiseren.\n",
    "\n",
    "* Door x- en y-coördinaten te genereren met _np.random.normal_ ontstaat er een _puntenwolk_ met als centrum (0,0). Door een constante waarde bij x of y te tellen kan je het centrum van deze wolk verschuiven in de x- of y-richting. Genereer nu volgende sample-data:\n",
    "  * een puntenwolk van 1000 instanties met als centrum (0,0) en color-label 'red'\n",
    "  * een puntenwolk van 1000 instanties met als centrum (2.5,2.5) en color-label 'green'\n",
    "  * een puntenwolk van 1000 instanties met als centrum (5,0) en color-label 'blue'\n",
    "  * bij de aanroep van _np.random.normal_ gef je geen extra parameters mee, tenzij de size\n",
    "  \n",
    "* Maak een scatter-plot van deze data. Als alles goed zit, zie je de 3 apart ingekleurde puntenwolken die lichtjes overlappen. (Je kan de color-kolom rechtstreeks doorgeven aan matplotlib.) Met volgende code kan je ervoor zorgen dat X en Y dezelfde schaal hanteren:\n",
    "\n",
    "    <code>plt.gca().set_aspect('equal', adjustable='box')</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Decision trees: visualisatie van de decision boundary**\n",
    "\n",
    "* Deel de data in in een training- en testset (70%/30%)\n",
    "* Train een DecisionTreeClassifier met de trainingsdata en meet de accuracy op de training- en testdata\n",
    "* We gaan nu de decision boundary benaderen door eerst de voorspelling op te vragen voor een grid van (x,y)-coördinaten die de volledige grafiek bedekt. Deze grid genereer je als volgt:\n",
    "\n",
    "<code>grid = np.mgrid[-4:8.6:0.05, -4:6:0.05].reshape(2,-1).T</code>\n",
    "\n",
    "* De voorspelde waarden kan je ook weer rechtstreeks doorgeven als kleur van de scatter-plot\n",
    "\n",
    "\n",
    "* Pas nu je script aan zodat je voor max_depth van 1 t.e.m. 8 de accuracies print en de decision boundary plot\n",
    "\n",
    "Kan je de decision boundary van max_depth=1 verklaren? Kan je de instelling met de beste bias-variance tradeoff ook visueel verklaren a.d.h.v. de decsion boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Random forests en gradient boosting machines**\n",
    "\n",
    "* Toon nu ook de accuracies en de decision boundary voor Random forests en gradient boosting machines\n",
    "* Random forests: waarom heeft parameter tuning van max_features hier geen zin?\n",
    "* Gradient boosting machines: experimenteer eens met de learning_rate ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
